---
title             : "Nonresponse and Sample Weighting in Organizational Surveying"
shorttitle        : "NONRESPONSE AND SAMPLE WEIGHTING"

author: 
  - name          : "John T. Kulas"
    affiliation   : "1"
    corresponding : yes
    address       : "Dickson Hall #250, Montclair State University, Montclair, NJ, 07043"
    email         : "kulasj@montclair.edu"
  - name          : "Yang Yang"
    affiliation   : "2"
  - name          : "David H. Robinson"
    affiliation   : "3"

affiliation:
  - id            : "1"
    institution   : "Montclair State University"
  - id            : "2"
    institution   : "Roche Group"
  - id            : "3"
    institution   : "St. Cloud State University"

author_note: |
  John T. Kulas, Professor of Industrial and Organizational Psychology, Department of Psychology, Montclair State University. Yang Yang, Organizational Scientist, People Insights & Technology, People & Culture, Roche Group. David H. Robinson, Professor of Statistics, St. Cloud State University. This manuscipt was generated via papaja [@R-papaja] within rMarkdown.

abstract: |
  Post-stratification weighting is a common procedure used in public opinion polling applications to correct demographic constituency differences between samples and populations. Although common practice in public opinion polling, this form of data remediation is only lightly acknowledged as a procedural topic worthy of empirical investigation within the organizational surveying literature. The current paper induces survey nonresponse via data simulation across fictional constituent groups (aka organizational strata) and documents the impact of weighting on the accuracy of sample statistics. Our goal was to evaluate the effectiveness of weighting when confronted with *passive* versus *active* forms of nonresponse in an effort to: 1) interject this data-remediation procedure into the established organizational surveying literature focused on nonresponse, while also 2) exploring organizationally-relevant sampling scenarios that: a) benefit from, b) are "hurt" by, or c) are relatively unaffected by post-stratification weighting. The results confirm that sampling contexts characterized by active nonresponse do benefit from application of sample weights, but only when accompanied by constituency differences in underlying population construct (e.g., surveyed *attitude*) standing. Alternatively, constituent member differences in population attitudes, when characterized by passive forms of nonresponse, exhibit no benefit from weighting (and in fact sample representativeness in these scenarios may be somewhat *hurt* by weighting). The simulations reinforce that, moving forward, it would be prudent for surveyors of all disciplinary backgrounds to mutually attend to the traditional focus of both traditions: public opinion polling (e.g., multiple possible  methodological sources of error as well as post-stratification adjustment) and organizational surveying (e.g., *form* of nonresponse). 
  
keywords          : "Survey methodology, sample weighting, nonresponse, response rate"

bibliography      : "simulation paper references.bib"

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : yes
header-includes:
  - \raggedbottom
#  - \usepackage{float} #use the 'float' package
#  - \floatplacement{figure}{H} #make every figure with caption = h
csl               : "apa7.csl"
documentclass     : "apa7"
classoption       : "man"
output            : papaja::apa6_pdf

#header-includes: #used to change page orientation
#- \usepackage{pdflscape}
#- \newcommand{\blandscape}{\begin{landscape}}
#- \newcommand{\elandscape}{\end{landscape}}
---

```{r chunk-preferences}
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, warning=FALSE, echo=FALSE)
```

```{r load_packages, include = FALSE}
library(papaja)   #APA6 formatting
# library(citr)     #APAcitation

library(psych)
library(ggplot2)  #data visualization
library(reshape2) #convert Wide to Long (Fig2&3)

library(Rmisc)    #summarySE()
library(weights)  #rd()

library(anesrake) #for citation - not used in body of paper - 2/1/23
```


```{r data load and prep, include=FALSE}

cond1_nnnn <- read.csv("norm_norm_norm_norm_summary.csv")
cond2_pppp <- read.csv("pos_pos_pos_pos_summary.csv")
cond3_gggg <- read.csv("neg_neg_neg_neg_summary.csv")
cond4_nnng <- read.csv("norm_norm_norm_neg_summary.csv")
cond5_nnpp <- read.csv("norm_norm_pos_pos_summary.csv")
cond6_pnng <- read.csv("pos_norm_norm_neg_summary.csv")
cond7_pgpg <- read.csv("pos_neg_pos_neg_summary.csv")
cond8_ppgg <- read.csv("pos_pos_neg_neg_summary.csv")

cond1_nnnn$cond <- "Condition 1"
cond2_pppp$cond <- "Condition 2"
cond3_gggg$cond <- "Condition 3"
cond4_nnng$cond <- "Condition 4"
cond5_nnpp$cond <- "Condition 5"
cond6_pnng$cond <- "Condition 6"
cond7_pgpg$cond <- "Condition 7"
cond8_ppgg$cond <- "Condition 8"

combo <- rbind(cond1_nnnn, cond2_pppp, cond3_gggg, cond4_nnng, cond5_nnpp, cond6_pnng, cond7_pgpg, cond8_ppgg)
names(combo)[names(combo)=="cond"] <- "Condition"


## Sampling Rate for four subgroups(MA,MB,FA,FB)
combo$cellrate.ma <- with(combo, SSmale*SSdepta)
combo$cellrate.mb <- with(combo, SSmale*SSdeptb)
combo$cellrate.fa <- with(combo, SSfemale*SSdepta)
combo$cellrate.gb <- with(combo, SSfemale*SSdeptb)
# SD index to indicate passive and active forms
combo <- transform(combo, SD=apply(combo[18:21],1, sd, na.rm = TRUE))


## Total response rate 
combo$RR.all <- with(combo, PPmale*PPdepta*SSmale*SSdepta + PPmale*PPdeptb*SSmale*SSdeptb +
                       PPfemale*PPdepta*SSfemale*SSdepta + PPfemale*PPdeptb*SSfemale*SSdeptb)


## Population and sample subgroup constituencies
combo$NS.ma <- with(combo, 10000*PPmale*PPdepta*SSmale*SSdepta)
combo$NS.mb <- with(combo, 10000*PPmale*PPdeptb*SSmale*SSdeptb)
combo$NS.fa <- with(combo, 10000*PPfemale*PPdepta*SSfemale*SSdepta)
combo$NS.fb <- with(combo, 10000*PPfemale*PPdeptb*SSfemale*SSdeptb)
combo$NS <- with(combo,NS.ma+NS.mb+NS.fa+NS.fb)

combo$cell.sma <- with(combo, NS.ma/NS)*100
combo$cell.smb <- with(combo, NS.mb/NS)*100
combo$cell.sfa <- with(combo, NS.fa/NS)*100
combo$cell.sfb <- with(combo, NS.fb/NS)*100
combo$cell.pma <- with(combo, PPmale*PPdepta)*100
combo$cell.pmb <- with(combo, PPmale*PPdeptb)*100
combo$cell.pfa <- with(combo, PPfemale*PPdepta)*100
combo$cell.pfb <- with(combo, PPfemale*PPdeptb)*100

#combo$PPma <- scale(combo$cell.pma)
#combo$PPmb <- scale(combo$cell.pmb)
#combo$PPfa <- scale(combo$cell.pfa)
#combo$PPfb <- scale(combo$cell.pfb)
#combo$TSma <- scale(combo$cell.sma)
#combo$TSmb <- scale(combo$cell.smb)
#combo$TSfa <- scale(combo$cell.sfa)
#combo$TSfb <- scale(combo$cell.sfb)
#combo$one     <- (combo$PPma - combo$TSma)^2
#combo$two     <- (combo$PPmb - combo$TSmb)^2
#combo$three   <- (combo$PPfa - combo$TSfa)^2
#combo$four    <- (combo$PPfb - combo$TSfb)^2

combo$one     <- (combo$cell.pma - combo$cell.sma)^2
combo$two     <- (combo$cell.pmb - combo$cell.smb)^2
combo$three   <- (combo$cell.pfa - combo$cell.sfa)^2
combo$four    <- (combo$cell.pfb - combo$cell.sfb)^2

#The value E is the chance expected median value of sum of squared differences and
#if the d is measured as differences in stand scores, we obtain Formular 2, E = 2k
combo$Chron.d2 <- with(combo, one+two+three+four)
combo$r.p      <- with(combo, (3.357*2-Chron.d2)/(3.357*2+Chron.d2))
summary(combo$r.p)


#Active and passive forms subsets
combo$ID <- c(1:nrow(combo))
combo.passive <- subset(combo, ID < 12289)     #Cond1to3
combo.active <- subset(combo, ID > 12288)     #Cond4to8

```

Akin to differential variable weighting (for instance: a) construct indicators within a multi-item assessment scale [aka factor loadings], or b) predictors within a selection system [aka regression weights]), sample weighting alters the proportional contributions of *individual respondents* within a data set. Some respondents' responses are assigned greater relative contribution and others are assigned less. This practice is commonplace in the summary of general population polling data reflecting, for example, elections and politics [e.g., @rivers_inference_2009], prevalence rates of psychological disorders [e.g., @kessler_national_2009], or feelings of physical safety [e.g., @quine_feeling_2008]. It is also seemingly in the periphery of awareness and interest within the published organizational surveying literature [see, for example, @kulas_post-stratification_2016; @landers_inconvenient_2015; @tett_2011_2014].

We speculate that this form of statistical remediation is gaining research interest in the organizational surveying research domain, at least in part, because industrial psychologists are keenly aware that response rates within organizational surveying applications have been trending downward [see, for example, @anseel_response_2010; @rogelberg_introduction_2007]. With lower response rates, surveyors are confronted with heightened levels of scrutiny because, historically, higher locally realized response rates have been interpreted as indicators of data quality [e.g., @anseel_response_2010; @cycyota_enhancing_2002; @cycyota_what_2006; @frohlich_techniques_2002]. The orientation of this presentation, however, is that although response rate is a commonly referenced proxy of survey quality, it is not response rate but rather sample representativeness that should be the primary focus of concern for survey specialists [see, for example, @cook_meta-analysis_2000; @krosnick_survey_1999]. Representativeness can of course be "hurt" by low response rates, but the relationship between these two survey concepts is by no means exact [e.g., @curtin_effects_2000; @keeter_gauging_2006; @kulas_nonresponse_2017]. Stated differently, a high response rate is neither a sufficient nor necessary condition for representative population sampling.[^1]

[^1]: Statistical benefits exist that are commonly attributed to higher response rates, such as greater power. These benefits, however, do not originate from response rate, but rather its consequence: larger *n*. Our presentation reflects the fact that greater power (and/or, relatedly, smaller confidence intervals) may in fact foster a false sense of confidence regarding “data quality”. Primarily for this reason, we stress that the methodological/statistical concepts of response rate, sample size, and power should be fully disentangled from the principle of representativeness, and the importance of these dissociations drives the central theme of the current paper.

In the context of survey applications, population misrepresentation refers to a discrepancy between estimated sample statistics and actual population parameters. Ideally, such discrepancies arise from completely random sources. In reality, however, discrepancies are driven not only by purely random causes. There are several broader sampling methodology factors that may be systematically driving the relative under- or over-selection of a population segment [see, for example, @kulas_post-stratification_2016], but the most commonly cited contributor within the organizational sciences is non-response [e.g., invited individuals simply either forget or consciously choose not to participate in the survey process, see, for example, @rogelberg_employee_2000]. Our presentation also focuses on this non-response contributor to sample misrepresentation, but only because we aim to: 1) integrate the organizational non-response and public-opinion  post-stratification weighting literatures, while also 2) highlighting the associations and dissociations between response rate and misrepresentation (although we note here that the focal procedure also addresses alternative methodological sources of misrepresentation).[^footie]

[^footie]: Frequently presented as a separate consideration, *measurement error* is an additional contributor to population misrepresentation and is not addressed via the weighting procedure. The concern of weighting is deviations from a perfect sampling methodology as opposed to deviations from an ideal psychometric methodology. We do however note that future advancements within the broad domain of "survey error" would benefit from a unified perspective that encompasses error arising from both methodological sources: measurement and sampling strategy. 

## Nonresponse in Organizational Surveying

Within the organizational surveying domain, it is not uncommon for response rate (RR) to be referenced as a proxy for survey data quality [see, for example, @baruch_survey_2008; @fan_factors_2010; @pedersen_improving_2016]. @baruch_response_1999, for example, states that, "...to have dependable, valid, and reliable results, we need a high RR from a wide representation of the whole population under study" and that, "The level of RR is an important, sometimes crucial, issue in relation to the validity of a paper's results" (p. 422). @fan_factors_2010 similarly state that response rate is, "...the most widely used and commonly computed statistic to indicate the quality of surveys" (p. 132). @pedersen_improving_2016 claim that a high survey response rate, "...diminishes sampling bias concerns and promotes the validity of survey-based research findings" (p. 230). The general consensus seems to be that there are three major (negative) consequences of low response rates, including (a) yielding smaller sample size, which negatively impacts statistical power and confidence intervals, (b) reducing the credibility of survey data, and (c) generating biased samples that impair the generalizability of survey results [@biemer_introduction_2003; @luong_how_1998; @rogelberg_employee_2000].

To the likely frustration of those who associate response rate with survey data quality, organizational survey response rates have, on average, been declining for decades. @baruch_response_1999, for example, summarized response rates of 175 studies published in five leading management and behavioral sciences journals in 1975, 1985, and 1995. His results revealed an average response rate (across time periods) of 55.6% (*SD* = 19.7%), but also a trend within which response rates declined steadily from 64.4% to 55.7% to 48.4% over the three time points. Nine years later, @baruch_survey_2008 conducted a follow-up study of 1,607 studies published in 17 disciplinary-relevant journals in 2000 and 2005 but found no substantial differences in response rates compared to those in 1995, suggesting that the declining trend had perhaps reached a lower asymptote.  However, a different approach with similar goals [@anseel_response_2010] analyzed 2,037 survey projects published in 12 journals in Industrial and Organizational Psychology, Management, and Marketing from 1995 to 2008 and did note a slight decline (overall *M* = 52.3%) when controlling for the use of response enhancing techniques.[^2] The most recent like-minded review focused on the years 2010, 2015, and 2020 and concluded that the trend had perhaps reversed, such that average response rates had risen to 68% in 2020 [@holtom2022survey].

[^2]: It is also possible that the declination had stabilized with mean response rates hovering around 50% after roughly the turn of the millenium [*M* = 52.5% for HRM studies from 2009 to 2013, @mellahi_response_2016; *M* = 52.0% for management studies from 2000 to 2004, @werner_reporting_2007]. This stability, if authentic, may again possibly be accounted for by an increased contemporary emphasis on response enhancing strategies [@anseel_response_2010; @fulton_organizations_2016].

### Form of Nonresponse

Although high response rates are considered desirable within organizational surveying applications, there has also been a broad acknowledgement that not all forms of nonresponse should be considered equally worrisome. @rogelberg_profiling_2003, for example, proposed a distinction between active and passive nonrespondents based on intent and (in)action. According to @rogelberg_profiling_2003, active nonrespondents are those who intentionally refuse to participate in surveys, while passive nonrespondents are those who fail to respond to surveys due to reasons such as forgetting or misplacing invitations. Passive nonrespondents are thought to be similar to respondents in both attitude [@rogelberg_profiling_2003] as well as organizational citizenship behaviors [OCBs, @spitzmuller_survey_2007], whereas active nonrespondents have been shown to exhibit significantly lower organizational commitment and satisfaction, higher intention to quit, lower conscientiousness, and lower OCBs than survey respondents [@rogelberg_employee_2000; @rogelberg_profiling_2003; @spitzmuller_survey_2007]. @taris_how_2007 similarly noted that selection of an individual population element into a realized sample may in fact be predictable (because of, for example, an increased likelihood of not responding when dissatisfied or disgruntled).

The more commonly encountered form of organizational nonresponse appears to be passive [@rogelberg_introduction_2007; @rogelberg_profiling_2003], although subgroup rates may evidence variability - men, for example, have a higher proclivity toward active nonresponse than do women [@luong_how_1998; @rogelberg_employee_2000; @spitzmuller_survey_2007].  The organizational surveying baseline default expectation is that, *on average*, roughly 15% of nonrespondents should be expected to be accurately characterized as "active" [@rogelberg_introduction_2007; @rogelberg_profiling_2003; @werner_reporting_2007]. It is this second, less frequently anticipated form of nonresponse that also carries the greater resulting threat of biased sample estimates [see, for example, @kulas_nonresponse_2017; @rogelberg_introduction_2007]. It is these biased estimates that are the desired target of remediation when applying sample weights.

# Sample Weighting - a Brief Overview

Within public opinion polling contexts, when realized sample constituencies (e.g., 44% male - by tradition from *carefully-constructed* and *randomly sampled* data frames)[^again] are compared against census estimates of population parameters (e.g., 49% male), weights are applied to the sample in an effort to remediate the relative proportional under- or over-sampling. This is because, if the  broader populations from which the under- or over-represented groups are sampled differ along surveyed dimensions (e.g., males, within the population, are *less likely to vote for Candidate X* than are women), then unweighted aggregate statistics (of, for example, projected voting results) will misrepresent the true population parameter. This remedial application of sample weights should also be considered an option for researchers pursuing answers to analagous organizational pollings such as: "What is the mood of the employees?" This is because focused queries such as this are of course covertly complex - implicit in the question is a focus not on survey results, but rather the broader employee population. Acknowledging the appropriate object of attribution is of course important, because the next step (after gauging the mood of the surveyed respondents) is *doing something* about it. Weighting may be a procedural option for organizational surveyors to credibly transition a bit closer from, "What do the survey results say"? to "What do the employees feel"?

[^again]: These important sampling concepts are very carefully attended to within public opinion polling contexts. Conversely, within organizational surveying traditions, these considerations are not commonly acknowledged, at least explicitly within the published literature. The weighting procedure presented in the current manuscript remediates bias regardless of the methodological source, but is dependent on accurate "census" population constituency estimates.   

## Procedural application

*Proportional weights* are the form of weights most directly relevant to organizational surveying applications that traditionally focus on nonresponse as the primary contributor to sample misrepresentation. These weights are ratios of the proportion of a population within a given stratum to the proportion of the sample within that same stratum:

\begin{equation}
proportional \: weight(\pi_k) = \frac{(N_k/N)}{(n_k/n)}
\end{equation}

Over-sampling of elements of a stratum (*k*) results in proportional weights less than one, while under-sampling (relative to the population) results in proportional weights greater than one.
The common procedure for weight estimation *when more than one stratum is specified* is an iterative process that may be referred to by multiple substantively synonymous terms: *rim weighting*, *iterative proportional fitting*, or *raking* [see, for example, @deming_least_1940]. Regardless of label, the procedure guides the surveyor to:

1) Determine proportional weights for all levels within one stratum, and then assign these weights to cases.

2) Determine proportional weights for a second group (ratio of population percent to *current* sample percent [the current sample percent will be affected by the step 1 weighting procedure]). Multiply previous (step 1) weights by the proportional weights for this second stratum and assign these new weights to cases.

3) Determine proportional weights for a third stratum (which will once again require re-inspection of the *current* sample percent). Multiply the previous step 2 weights by the third stratum proportional weights and assign to cases.
  
4) Iterate through steps 1, 2, and 3 (or more if more than three strata are considered) until the weighted sample characteristics match the population characteristics to your desired level of precision. 

Possible strata relevant for organizational survey weighting include: branch, full-, part-, or flex-time status, functional area, gender, geographic location, hierarchy, remote-working categorization, salaried status, subsidiary, tenure, work shift, or any other groupings especially suspected to plausibly possess a relatively disporportionate number of active nonrespondents (through application of forecasting strategies such as those advocated by, for example, Rogelberg and Stanton, 2007). Each of these strata may of course also be the targeted focus of survey results feedback, but when *aggregating* results across (or even within) strata, a consideration of the impact of nonresponse *has the potential* to yield more accurate survey estimates. The explicit goal is therefore a closer approximation of population parameters with descriptive sample statistics via statistical remediation, and drives the current paper's focus on the interplay of four survey elements: 1) response rate, 2) nonresponse form, 3) distribution of attitude within the larger population, and 4) remedial weighting.

*Research question 1*: What role does response rate play in population misrepresentation? 

*Research question 2*: What role does nonresponse form (passive versus active) play in population misrepresentation? 

*Research question 3*: What impact does the application of weights have on both biased[^whatchathink] and unbiased sample estimates? 

We view these questions as being analogous to similar questions asked and answered regarding differential *variable* weighting within the broader applied psychological disciplines. Just as, for example, there has been debate regarding the merits of differential versus unit variable weighting in a selection context or aggregate scale score definition [e.g., @bobko_usefulness_2007; @wainer_estimating_1976], we propose that a similar consideration is appropriate with persons, and therefore compare and contrast unit versus proportional sample weighting. 

[^whatchathink]: We have to be careful about the use of the term "bias" - either very carefully distinguish between error and bias or just avoid use of the term altogether. Perhaps Dr. Robinson can help here.

# Methods

We address our research questions within a simulated fictionalized context of organizational surveying (wherein it is common to assess estimates of employee attitude or perception; for example, commitment, culture/climate, engagement, satisfaction). We began the simulations by establishing "populations", each consisting of 10,000 respondents characterized by demographic categorizations across gender (male and female) and department (A and B). We therefore had four demographic groups (Male.A, Male.B, Female.A, and Female.B). For these population respondents, we generated scaled continuous responses (real numbers) ranging from values of 1 to 5, representing averaged aggregate scale scores from a fictional multi-item survey with a common 1 $\rightarrow$ 5 Likert-type rating scale.

```{r example1, fig.cap="Visual demonstrating terms used to describe population elements."}

## need to read in as figure
## https://docs.google.com/presentation/d/1nW-8azHZop82vByLDqdtqOJRTFAxa-vJ1sPgw5d9XPs/edit#slide=id.p

library(magick)
library(grid)

figure <- image_read("example.JPG")

grid.raster(figure)

```

In order to represent different proportions of relative constituency (for example, more Females than Males or more Department A workers than Department B), we iterated population characteristics at marginal levels (gender and department) starting at 20% (and 80%) with increments and corresponding decriments of 20%. For example, if Males accounted for 20% of the simulated population, then Females were 80%; also if respondents in Department A represented 60% of a population, then 40% were in Department B. Marginal constituencies were therefore realized at all combinations (across the two variables) of 20% and 80%, 40% and 60%, 60% and 40%, and 80% and 20%. This resulted in population *cell* constituencies (e.g., Male.A, Female.A, Male.B, Female.B) as low as 400 and as high as 6,400 - see Figure \@ref(fig:example1) for further clarification of our "cell" and "margin" terminology and relative constituency specification.

```{r Tab1, echo=FALSE, results="asis", eval=TRUE}

a <- c("1","","","2","","","3","","","4","","","5","","","6","","","7","","","8","","")
b <- c("Normal","Positive Skew","Negative Skew","Normal","Positive Skew","Negative Skew",
       "Normal","Positive Skew","Negative Skew","Normal","Positive Skew","Negative Skew",
       "Normal","Positive Skew","Negative Skew","Normal","Positive Skew","Negative Skew",
       "Normal","Positive Skew","Negative Skew","Normal","Positive Skew","Negative Skew")
c <- c("3","2","4","3","2","4","3","2","4","3","2","4",
       "3","2","4","3","2","4","3","2","4","3","2","4")
d <- c("X","","","","X","","","","X","X","","",
       "X","","","","X","","","X","","","X","")
e <- c("X","","","","X","","","","X","X","","",
       "X","","","X","","","","","X","","X","")
f <- c("X","","","","X","","","","X","X","","",
       "","X","","X","","","","X","","","","X")
g <- c("X","","","","X","","","","X","","","X",
       "","X","","","","X","","","X","","","X")
h <- c("Low","","","Low","","","Low","","",
       "Moderate","","","Moderate/High","","",
       "Moderate/High","","","High","","",
       "High","","")

tab.1 <- cbind.data.frame(a,b,c,d,e,f,g,h)

apa_table(tab.1,
          align=c("c","l",rep("c",6)),
          escape=TRUE,
          caption="Attitudinal Distribution Conditions Specified in Current Paper",
          col.names=c("Condition","Distributional Shape","mu","Dept A","Dept B","Dept A","Dept B","Bias Susceptibility"),
          col_spanners=list("Male"=c(4,5), "Female"=c(6,7)),
          small=TRUE,
          landscape=FALSE)

```

Each population cell was characterized by an attitudinal distribution in one of three different possible forms: normal, positively skewed, or negatively skewed. These distributional forms were retained in an attempt to model similarities and discrepancies in construct standing (e.g., commitment, satisfaction, or engagement) across localized respondent groupings. The normal distribution exhibited, on average, a mean of 3.0 whereas the skewed distributions were characterized by average means of 2.0 and 4.0, respectively. In total, eight crossings of distributional type across employee categorization were specified (Table \@ref(tab:Tab1) presents the combinations of these distributions). Note that these eight conditions are not exhaustive of all possible combinations of constituent groups and attitudinal distribution - we limited the simulations to combinations projected to collectively be most informative within the manipulated simulation parameters. 

Individual attitudes were randomly sampled from population distributions at the cell level (e.g., Male.A) without replacement. These response rates (methodologically within the simulation these could equally be conceptualized as *sampling* rates) were specified at 10% increments ranging from 60% to 90%, and these were fully iterated across each of our four marginal groups (Males, Females, Departments A and B). Our cell-level response rates therefore ranged from 36% to 81% - a range of rates that encompass reasonable real-world expectations according to the organizational surveying literature [e.g., @mellahi_response_2016; @werner_reporting_2007]. We therefore investigated error within the aggregate mean (e.g., grand mean aka total sample mean) attributable to different likelihoods of sample inclusion from constituent groups of different relative size and representing populations of different attitudinal distribution, but at response rates reasonably expected to exist in real-world organizational surveying contexts.  

It should be noted here that our operationalization of active versus passive utilizes *consistency* of response rate as a baseline indicator of passive nonresponse. There are several patterns of response that are therefore intended to represent sampling scenarios reflecting passive nonresponse across groups, *regardless of response rate*.  These are the scenarios in which all subgroups exhibit the same response rate (e.g., 36%, 36%, 36%, and 36%). All other combinations of response rate are intended operationalizations of active forms of nonresponse (e.g., not *as reasonably* characterized as missing at random).  

```{r Tab2, echo=FALSE, results="asis", eval=TRUE}

aa <- c("36%","36%","48%","42%","48%","56%","54%","63%","36%",
	"42%","49%","48%","56%","36%","64%","42%","36%","42%",
	"48%","42%","49%","36%","48%","56%","36%","36%","42%",
	"42%","49%","42%","48%","36%","48%","54%","42%","36%")

bb <- c("36%","36%","48%","42%","48%","56%","54%","63%","42%",
	"48%","56%","54%","63%","36%","72%","42%","42%","49%",
	"48%","48%","56%","36%","54%","63%","48%","42%","42%",
	"54%","63%","48%","48%","48%","54%","54%","54%","54%")

cc <- c("36%","42%","54%","49%","56%","64%","63%","72%","42%",
	"49%","56%","56%","64%","48%","72%","56%","48%","54%",
	"64%","56%","63%","54%","64%","72%","48%","54%","63%",
	"56%","63%","63%","72%","54%","72%","81%","63%","54%")

dd <- c("36%","42%","54%","49%","56%","64%","63%","72%","49%",
	"56%","64%","63%","72%","48%","81%","56%","56%","63%",
	"64%","64%","72%","54%","72%","81%","64%","63%","63%",
	"72%","81%","72%","72%","72%","81%","81%","81%","81%")



ee <- c(".000",".034",".035",".040",".046",".047",".051",".052",".053",
	".057",".061",".062",".066",".069",".069",".081",".085",".089",
	".092",".096",".098",".104",".106",".109",".115",".120",".121",
	".123",".131",".137",".139",".150",".154",".156",".164",".186")


ff <- c("256","128","64","192","128","64","128","64","64",
	"128","64","128","128","128","64","128","128","128",
	"128","128","128","192","128","128","64","128","64",
	"128","64","128","64","128","128","64","128","64")


gg <- c("Passive","","","","","","","","",
        "","","","","","","","","",
        "","","","","","","","","",
        "","","","","","","","","Active")
tab.2 <- cbind.data.frame(aa,bb,cc,dd,ee,ff,gg)

apa_table(tab.2,
          align=c(rep("c",7)),
          caption="Example Summarized Response Rate Conditions Represented in Figures 2 through 5",
          col.names=c("Male Dept A","Male Dept B","Female Dept A","Female Dept B","SD Index", "Number of Conditions","Form (and degree) of Nonresponse"),
          col_spanners=list("Example Response Rates (Any Combination)"=c(1,4)),
          longtable = TRUE,
          small=TRUE,
          landscape=TRUE)
```

In an attempt to capture the "degree of active nonresponse", we calculated a simple index of response rate discrepancy (SD; presented in Table \@ref(tab:Tab2)). The "least" active nonresponse scenarios are characterized by two subgroups with identical response rates and two  having a slightly different response rate (e.g., male.a = 36%, female.a = 36%, male.b = 42%, and female.b[^sample] = 42%; see the second row of Table \@ref(tab:Tab2), the SD index = .034)[^3]. Also here note that three of our eight Table \@ref(tab:Tab1) conditions represent scenarios where the presence of active nonrespondents is not expected to result in bias (e.g., regardless of patterns of nonresponse, the unweighted sample mean is expected to yield an unbiased estimate of the population mean). These are Table \@ref(tab:Tab1) conditions one through three, where attitudinal distributions are of *the same form* across groups, regardless of any individual group response rate discrepancy from others'.

[^sample]: Throughout the Method and Results, "lowercase" specification of simulation strata indicates sample constituencies (e.g., male.b) whereas uppercase implicates population (e.g., Male.B).

[^3]: This method of simplifying the presentation of our response rate conditions is fully independent of consideration of population constituency and distributional form. That is, the amount of bias present in a sample estimate is expected to be quite different for Condition 7 with response rates of 48%, 48%, 72%, 72% versus 48%, 72%, 48%, 72%, even though the crude response rate index (SD = 0.139) is the same for both scenarios.  There is additional information within these simulations (the effect of a *combination* of response rate and population form on degree of bias) that is therefore not captured via this simple SD index. 

The operationalizations of passive and active forms of nonresponse retained here differ from other investigations with similar goals. @kulas_nonresponse_2017, for example, directly tie probabilities of sample inclusion to an individual's held attitude (the likelihood of sample inclusion is fully dependent on the population member's attitude). Conversely, the probability of sample inclusion in the current investigation is dependent only on *group* membership (with some of these groups occasionally being characterized by unique attitude distributional forms). Essentially, @kulas_nonresponse_2017 operationalize active nonresponse at the person-level whereas the current paper does so at the group level. This may be a more practical procedural specification with regard to the implications of these simulations, as organizational surveyors are more likely to have an inclination of a group's collective attitude or likelihood to respond (e.g., night shift workers, machine operators) than they are of any one individual employee.

# Results

```{r ResponseRate1, echo=FALSE, fig.cap="Relationship between total response rate and misrepresentation.", warning=FALSE, fig.height=4.5, fig.width=6.5}

ggplot(combo, aes(x=RR.all, y=PPSU)) + 
  geom_point(alpha=1/25, size=1.5) + 
  xlim(0.36,0.81) +
  ylim(0, .3) + 
  xlab("Total Response Rate") + 
  ylab("Misrepresentation") +
  theme(panel.background = element_rect(fill = "transparent", color = "#CCCCCC"),
        panel.grid.major.y = element_line(color = "grey80")) + 
  scale_colour_gradient() +
  geom_smooth(method='loess', color="red", size = 1/3) +
  theme(axis.title.x = element_text(size = 8,color = "black")) +
  theme(axis.text.x = element_text(size = 8, color = "black")) +
  theme(axis.title.y = element_text(size = 8,color = "black")) +  
  theme(axis.text.y = element_text(size = 8,color = "black")) +  
  facet_wrap(~Condition) +
  theme(strip.text.x = element_text(size = 10,color= "black"))

copycombo <- combo
copycombo$Condition <- stringr::str_replace_all(copycombo$Condition, " ", "")
rrdesc <- psych::describeBy(PPSU ~ Condition, data = copycombo)

rr.aov <- aov(PPSU~Condition, copycombo)  

copycombo2 <- copycombo[ which(copycombo$Condition == "Condition4" |  
                                 copycombo$Condition == "Condition5" |
                                 copycombo$Condition == "Condition6" |
                                 copycombo$Condition == "Condition7" |
                                 copycombo$Condition == "Condition8"),]

polynomial <- lm(PPSU ~ Condition, copycombo2)
polynomial1 <- lm(PPSU ~ Condition + RR.all, copycombo2)
polynomial2 <- lm(PPSU ~ Condition + RR.all + I(RR.all^2), copycombo2)

firstpoly <- anova(polynomial,polynomial1)
secondpoly <- anova(polynomial1,polynomial2)

lm.first <- summary(polynomial)$r.squared       ## was rendering odd in inline script
lm.second <- summary(polynomial1)$r.squared
lm.third <- summary(polynomial2)$r.squared

# summary(polynomial2)
  
```

```{r SDForm2, echo=FALSE, fig.cap="Relationship between nonresponse form and misrepresentation.",fig.height=4.5, fig.width=6.5}

ggplot(combo, aes(x=SD, y=PPSU)) + 
  geom_point(alpha=1/25, size=1.5) + 
  xlim(0, .2) +
  ylim(0, .3) + 
  xlab("SD indicating Form of Nonresponse") + 
  ylab("Misrepresentation") +
  theme(panel.background = element_rect(fill = "transparent", color = "#CCCCCC"),
        panel.grid.major.y = element_line(color = "grey80")) + 
  scale_colour_gradient() +
  geom_smooth(method='loess', color="red",size=1/3) +
  theme(legend.title = element_text(size = 8)) +
  theme(legend.text = element_text(size = 8)) +
  theme(axis.title.x = element_text(size = 8,color = "black")) +
  theme(axis.text.x = element_text(size = 8, color = "black")) +
  theme(axis.title.y = element_text(size = 8,color = "black")) +
  theme(axis.text.y = element_text(size = 8,color = "black")) +
  facet_wrap(~Condition) +
  theme(strip.text.x = element_text(size = 10,color= "black"))

```

```{r Cattell3, echo=FALSE, fig.cap="Effect of subgroup sampling rate match with distributional form on population  misrepresentation.",fig.height=4.5, fig.width=6.5}

ggplot(combo, aes(x=r.p, y=PPSU)) + 
  geom_point(alpha=1/25,size=1.5) + 
  xlim(-1.0, 1.0) + 
  ylim(0, .3) + 
  xlab("Cattell's Profile Similarity Coefficient rp") + 
  ylab("Misrepresentation") +
  theme(panel.background = element_rect(fill = "transparent", color = "#CCCCCC"),
        panel.grid.major.y = element_line(color = "grey80")) + 
  scale_colour_gradient() +
  geom_smooth(method='loess', color="red",size=1/3) +
  theme(axis.title.x = element_text(size = 8,color = "black")) +
  theme(axis.text.x = element_text(size = 8, color = "black")) +
  theme(axis.title.y = element_text(size = 8,color = "black")) +  
  theme(axis.text.y = element_text(size = 8,color = "black")) +
  facet_wrap(~Condition) +
  theme(strip.text.x = element_text(size = 10,color= "black"))

poly <- lm(PPSU ~ Condition, copycombo2)
poly1 <- lm(PPSU ~ Condition + r.p, copycombo2)
poly2 <- lm(PPSU ~ Condition + r.p + I(r.p^2), copycombo2)

firstpoly <- anova(poly,poly1)
secondpoly <- anova(poly1,poly2)

lm.1 <- summary(poly)$r.squared       ## was rendering odd in inline script
lm.2 <- summary(poly1)$r.squared
lm.3 <- summary(poly2)$r.squared

```

```{r Overall4, echo=FALSE, fig.cap="Average absolute discrepancy (unweighted in white and weighted in grey) across the eight attitudinal conditions.",fig.height=4.5, fig.width=6.5}

combo.fig4 <- data.frame(Misrepresentation=numeric(), 
                        Condition=factor(levels = c("Condition 1","Condition 2","Condition 3","Condition 4",
                                                    "Condition 5","Condition 6","Condition 7","Condition 8")),
                        Type=factor(levels = c("Misrepresentation before applying weighting",
                                               "Misrepresentation after applying weighting")))

combo.fig4[c(1:32768),c(1:2)] <- combo[c(1:32768),c(14,2)]
combo.fig4[c(1:32768),3]   <- "Misrepresentation before applying weighting"
combo.fig4[c(32769:65536),c(1:2)] <- combo[c(1:32768),c(15,2)]
combo.fig4[c(32769:65536),3]   <- "Misrepresentation after applying weighting"

combo.fig4 <- summarySE(combo.fig4, measurevar="Misrepresentation", groupvars=c("Type","Condition"))

ggplot(combo.fig4, aes(x=Condition, y=Misrepresentation, fill=Type)) + 
  xlab("Simulation Conditions") + 
  ylab("Mean Misrepresentation") +
  scale_y_continuous(limits = c(0, .15), breaks=seq(0,.15, by =.01)) + 
  geom_bar(position=position_dodge(), stat="identity", colour="black") +
  geom_errorbar(aes(ymin=Misrepresentation-sd,ymax=Misrepresentation+sd),
                    width=.2,position=position_dodge(.9)) +
  scale_fill_manual(values=c("#FFFFFF","#999999")) +
  theme(legend.justification = c(0,1), 
        legend.position = c(.1,.97),
        legend.title = element_blank(),
        legend.text = element_text(size = 8,colour = "black"),
        legend.background = element_rect(fill="transparent")) +
  theme(axis.title.x = element_text(size = 8,colour = "black")) + 
  theme(axis.title.y = element_text(size = 8,colour = "black")) +  
  theme(axis.text.x = element_text(size = 8,colour = "black")) +
  theme(axis.text.y = element_text(size = 8,colour = "black")) +
  theme(panel.background = element_rect(fill = "transparent"),
        panel.border = element_rect(colour = "grey80", fill=NA, size=0.5),
        panel.grid.major.y = element_line(colour = "grey80"))

```

```{r Together5, echo=FALSE, fig.cap="Presence (or lack) of error in unweighted and weighted sample estimates across passive and active forms of nonresponse (Conditions 1 through 3).",fig.height=7, fig.width=6.5, include=FALSE}

## deleted from paper 6/15 because of seeming redundancy with Figure 2

combo.fig5 <- combo.passive[,c(2,14,15,22)]
combo.fig5 <- melt(combo.fig5, id.vars=c("Condition","SD"), variable.name="type", value.name="difvalue")

rownum <- nrow(combo.fig5)/2

for (i in 1:rownum){
  combo.fig5$type2[i] <- "Before applying weighting"
  combo.fig5$type2[i+rownum] <- "After applying weighting"
}

rm(rownum,i)  #delete extraneous objects

xlab <- expression('SD Index')

ggplot(combo.fig5, aes(x=SD, y=difvalue, color=Condition)) + 
  geom_point(alpha=1/2,size=1.5)+ 
  xlim(0,.2) + 
  ylim(0,.4) + 
  xlab("SD Index") + 
  ylab("Misrepresentation") +
  #geom_jitter(width=.03) +
  theme(panel.background = element_rect(fill = "transparent", colour = "#CCCCCC"),
        panel.grid.major.y = element_line(colour = "grey80")) + 
  theme(axis.title.x = element_text(size = 8,colour = "black")) +  
  theme(axis.title.y = element_text(size = 8,colour = "black")) +  
  theme(axis.text.x = element_text(size = 8,colour = "black")) +
  theme(axis.text.y = element_text(size = 8,colour = "black")) +
  theme(legend.title = element_blank(),
        legend.text = element_text(size = 8, colour = "black"),
        legend.background = element_rect(fill="transparent")) +
  #geom_vline(xintercept=.025) + 
  scale_colour_brewer() + 
  annotate("text", x=.00, y=.35, label="Passive", color = "black", size = 3) +
  annotate("text", x=.20, y=.35, label="Active", color = "black", size = 3) +
  geom_smooth(method=lm, se=FALSE, colour="red", size = 1/3) +
  facet_wrap(~type2, nrow=2) +
  theme(strip.text.x = element_text(size=10,colour="black"))

```

```{r Together6, echo=FALSE, fig.cap="Presence (or lack) of error in unweighted and weighted sample estimates across passive and active forms of nonresponse (Conditions 4 through 8).",fig.height=7, fig.width=6.5, include=FALSE}

combo.fig6 <- combo.active[,c(2,14,15,22)]
combo.fig6 <- melt(combo.fig6, id.vars=c("Condition","SD"), variable.name="type", value.name="difvalue")

rownum <- nrow(combo.fig6)/2

for (i in 1:rownum){
  combo.fig6$type2[i] <- "Before applying weighting"
  combo.fig6$type2[i+rownum] <- "After applying weighting"
}

rm(rownum,i)  #delete extraneous objects

xlab <- expression('SD Index')

ggplot(combo.fig6, aes(x=SD, y=difvalue, color=Condition)) + 
  geom_point(alpha=1/2,size=1.5)+ 
  xlim(0,.2) + 
  ylim(0,.4) + 
  xlab("SD Index") + 
  ylab("Misrepresentation") +
  #geom_jitter(width=.03) +
  theme(panel.background = element_rect(fill = "transparent", colour = "#CCCCCC"),
        panel.grid.major.y = element_line(colour = "grey80")) + 
  theme(axis.title.x = element_text(size = 8,colour = "black")) +  
  theme(axis.title.y = element_text(size = 8,colour = "black")) +  
  theme(axis.text.x = element_text(size = 8,colour = "black")) +
  theme(axis.text.y = element_text(size = 8,colour = "black")) +
  theme(legend.title = element_blank(),
        legend.text = element_text(size = 8, colour = "black"),
        legend.background = element_rect(fill="transparent")) +
  #geom_vline(xintercept=.025) + 
  scale_colour_brewer() + 
  annotate("text", x=.00, y=.35, label="Passive", color = "black", size = 3) +
  annotate("text", x=.20, y=.35, label="Active", color = "black", size = 3) +
  geom_smooth(method=lm, se=FALSE, colour="red", size = 1/3) + 
  facet_wrap(~type2, nrow=2) +
  theme(strip.text.x = element_text(size=10,colour="black"))

```

In total, we generated 327.68 million samples (4,096 unique combinations of response rate and population constituencies across gender and department, simulated 10,000 times each across the eight Table \@ref(tab:Tab1) conditions).  Each of these samples was comprised of, on average, *n* = `r format(mean(combo[,"NS"]), big.mark=",", digits=2,scientific=FALSE)`, collectively representing an experiment-wide simulated *n* of 1.8432 trillion. For each individual simulation, weights were applied iteratively to the data at the two marginal (variable) levels via raking, and were estimated via the *anesrake* package [@R-anesrake] in `r R.version.string`. 

We were most interested in comparing the extent to which unweighted (aggregated responses without raking) and weighted (aggregated weighted) sample means approximated the known population means across our controlled specifications of response rate, nonresponse form, and attitudinal distribution. Population means were extracted from each iteration, as the simulations specified a new population at each iteration. "Misrepresentation" between sample and population was operationalized as: 1) the discrepancies between the population and both weighted and unweighted sample means, as well as, 2) the averaged deviation of these discrepancies from the population mean (discrepancy in the "mean" of the means is bias, dispersion about the "mean" of the means is error). If the average weighted sample mean was closer to the true population mean, relative to the unweighted one, then the weighting was deemed beneficial.[^butbut]

[^butbut]: Do we want to do a little more with the dispersion concept? Currently it's underreported in the Results (but stated here that it is something we look at). If so, do we say that the weighting was beneficial also if the dispersion (error) was relatively small? Probably need Dr. Robinson to weigh in on this one

# Unweighted effects

## Role of response rate

Research question 1 asked what singular effect response rate has on population misrepresentation. This is presented most concisely in Figure \@ref(fig:ResponseRate1), with *moderate* response rates exhibiting the greatest degrees of misrepresentation across our simulated conditions. Note here again that conditions 1 through 3, which represent populations with similar distributions of attitude, do not exhibit misrepresentation regardless of response rate ($\bar{d}_{Cond1}$ = `r round(rrdesc$Condition1$mean,3)`, $sd_{Cond1}$ = `r round(rrdesc$Condition1$sd,3)`; $\bar{d}_{Cond2}$ = `r round(rrdesc$Condition2$mean,3)`, $sd_{Cond2}$ = `r round(rrdesc$Condition2$sd,3)`; $\bar{d}_{Cond3}$ = `r round(rrdesc$Condition3$mean,3)`, $sd_{Cond3}$ = `r round(rrdesc$Condition3$sd,3)`). These can be contrasted most particularly with conditions 6 ($\bar{d}_{Cond6}$ = `r round(rrdesc$Condition6$mean,3)`, $sd_{Cond6}$ = `r round(rrdesc$Condition6$sd,3)`), 7 ($\bar{d}_{Cond7}$ = `r round(rrdesc$Condition7$mean,3)`, $sd_{Cond7}$ = `r round(rrdesc$Condition7$sd,3)`), and 8 ($\bar{d}_{Cond8}$ = `r round(rrdesc$Condition8$mean,3)`, $sd_{Cond8}$ = `r round(rrdesc$Condition8$sd,3)`), which evidence considerable misrepresentation, particularly so at moderate response rates (the greatest degree of misrepresentation occurs with aggregate response rates ranging from roughly 40% to 70%)[^confound]. Note also that all conditions exhibit circumstances where low and moderate response rates result in no misrepresentation. 

Discrepancies in unweighted means between samples and populations -- regardless of response rate -- did exhibit differences across the 8 conditions ($F_{(7,32,760)}$ = `r summary(rr.aov)[[1]][1,4]`, *p* < .001). Tukey's HSD implicated all contrasts other than between Conditions 1, 2, and 3 and also between Conditions 7 and 8. Retaining only Conditions 4 through 8, the relationship between response rate and sample/population discrepancy was estimated via hierarchical regression (step 1 = condition, step 2 = response rate, step 3 = squared response rate). In these analyses there was a significant but trivial linear response rate influence *beyond* the effect of condition ($\Delta{R^2}$ = `r round(lm.second-lm.first,4)`; $F$ = `r firstpoly[2,5]`), with the polynomial response rate term further adding slightly to the discrepancy prediction ($\Delta{R^2}$ = `r round(lm.third-lm.second,4)`; $F$ = `r secondpoly[2,5]`). Collectively these results reflect inconsistent direct relationships between response rate and population misrepresentation -- a range of representative/error-filled estimates were encountered all along the response rate continuum. The next sections explore potential explanatory mechanisms for these ranges of misrepresentation at identical rates of response.  

[^confound]: Note that extreme overall rates (e.g., .36/.81) are necessarily associated with more passive forms of non-response as operationalized in the current paper. The "middle"-most response rates are those most likely to be characterized by a mixture of both passive and active forms of non-response.

## Role of nonresponse *form*

Research question 2 asked what role the *form* of nonresponse (passive versus active) plays in population misrepresentation. In terms of explaining the error that did emerge within unweighted means sampled from conditions 4 though 8, this error was largely attributable to form of nonresponse as operationalized by our SD index (See Figure \@ref(fig:SDForm2)). Figure \@ref(fig:SDForm2) also adds context to the previously noted Figure \@ref(fig:ResponseRate1) variabilities in ranges of misrepresentation across response rates, with the most extreme Figure \@ref(fig:SDForm2) cases of misrepresentation fully echoing circumstances of active nonresponse (e.g., the greatest cases of misrepresentation are always associated with the highest SD index regardless of simulation condition). 

```{r, correcting.51}
## just trying to correct the ".51" estimate in the below paragraph - 1/10/24

## PPSU and PPSW are |absolute differences|
## PPSU2 and PPSW2 are actual differences

onlyeight <- combo[ which(combo$Condition == "Condition 8"), ]
onlyeight2 <- onlyeight[ which(onlyeight$SD > 0.154 & onlyeight$SD < 0.156),]

paralleleight <- onlyeight[ which(onlyeight$cellrate.ma == .54 &
                                    onlyeight$cellrate.mb == .54 &
                                    onlyeight$cellrate.fa == .81 &
                                    onlyeight$cellrate.gb == .81), ]

wrongeight <- onlyeight[ which(onlyeight$cellrate.ma == .54 &
                                    onlyeight$cellrate.mb == .81 &
                                    onlyeight$cellrate.fa == .54 &
                                    onlyeight$cellrate.gb == .81), ]
## Note - looked initially like 7 & 8 were mislabeled - Confirmed with Yang on 2/14/25 that condition 7 and 8 are correctly labeled - will need Dr. Robinson to help explain effect
```

```{r CattelExplain, fig.cap="Allocation of response rates relative to underlying distributional form and its impact on population misrepresentation (need to think through hi/lo given Dr Robinsons thoughts)"}

cattelexample <- data.frame(
  rr = c(.81,.81,.54,.54,.6,.75,.6,.75),
  group = as.factor(c("Male.A","Male.B","Female.A","Female.B","Male.A","Male.B","Female.A","Female.B")),
  Impact = as.factor(c("High","High","High","High","Low","Low","Low","Low")),
  image=c("posskew.png","posskew.png","negskew.png","negskew.png","posskew.png","posskew.png","negskew.png","negskew.png")
  )

cattelexample$group <- factor(cattelexample$group, levels = c("Male.A","Male.B","Female.A","Female.B"))

ggplot(cattelexample, aes(x=group, y=rr, group=Impact)) + 
  geom_line(aes(linetype=Impact)) +
#  geom_point() + 
  ylim(.5, .9) + 
  xlab("Sampled Element") + 
  ylab("Response Rate") +
  theme(panel.background = element_rect(fill = "transparent", color = "#CCCCCC"),
        panel.grid.major.y = element_line(color = "grey80")) + 
  scale_colour_gradient() +
  theme(axis.title.x = element_text(size = 8,color = "black")) +
  theme(axis.text.x = element_text(size = 8, color = "black")) +
  theme(axis.title.y = element_text(size = 8,color = "black")) +  
  theme(axis.text.y = element_text(size = 8,color = "black")) +
  ggimage::geom_image(
    aes(image = image),
    size = 0.2
  )

```

The Figure \@ref(fig:SDForm2) scatterplots also reveal patterned heteroskedasticity across the active nonresponse continuum. Similar to the response rate -- misrepresentation associations, there are *active nonresponse* scenarios in which no misrepresentation occurs (see, for example, the lower right-hand portions of conditions 4 through 8 where discrepancy estimates of "0" persist at multiple points along the passive--active x-axis). These circumstances are simulated conditions within which the response rate patterns "do not mirror" the *population distributional form*. For example, in Condition Eight, the distributional forms across populations were: $Positive Skew_{Male(A)}$, $Positive Skew_{Male(B)}$, $Negative Skew_{Female(A)}$, $Negative Skew_{Female(B)}$. Response rates that "track along with" distributional patterns, when also characterized by extreme cases of active nonresponse (e.g., SD = .156; 54%~Male(A)~, 54%~Male(B)~, 81%~Female(A)~, 81%~Female(B)~), result in *substantial error* in the population mean approximation (average discrepancy = `r mean(paralleleight$PPSU)`, *SD* = `r sd(paralleleight$PPSU)`). Alternatively, when the response rates are "not aligned" with distributional patterns for the SD=.156 cases, (e.g., 54%~Male_A~, 81%~Male_B~, 54%~Female_A~, 81%~Female_B~), there is very little error in approximation (average discrepancy = `r mean(wrongeight$PPSU)`, SD = `r sd(wrongeight$PPSU)`; See Figure \@ref(fig:CattelExplain) for visual reference). Here, it is not simply response rate or form that is associated with biased sample estimates, but rather the nature of response rate relative to existing attitudinal differences. 

### Attitudinal distribution and patterns of nonresponse

To further expand upon this *attitudinal form/pattern of nonresponse* interplay, the discrepancies between population constituency and sampling proportions were additionally evaluated through the lens of Cattell's profile similarity index [$r_p$, @cattell_r_1949; @cattell_taxonometric_1966]. $r_p$ is sensitive to discrepancies in profile shape (pattern across profile components), elevation (average component score), and scatter (sum of individual components' deviation from the elevation estimate. Here, the profile similarity index references the relationship between the response rates (NEED YANG TO VERIFY - THINK THIS IS SSmale;SSfemale;SSdepta;SSdeptb from `combo` object) and sample sizes (cellrate.ma;cellrate.mb;cellrate.fa;cellrate.gb) across experimental *cells*. For example, VERIFY BEFORE CLARIFYING HERE. Figure \@ref(fig:Cattell3) demonstrates the pattern of unweighted sample mean deviation (from the population parameter) when this index is taken into consideration. Specifically, Figure \@ref(fig:Cattell3) demonstrates a more pronounced *form of* nonresponse association when underlying attitudinal distributions evidence group differences (e.g., incrementally across the 8 specified conditions), and in these scenarios, active nonresponse is shown to have a fairly large effect on error within the sample estimate (as well as systematically increasing degrees of  heteroskedasticity paralleling the Cattell index; omnibus  Breusch-Pagan [across conditions] = 3177.2, *p* < .001). The curvilinear nature of these functions was estimated via hierarchical polynomial regression (excluding conditions 1, 2, and 3), with misrepresentation exhibiting a linear association across condition ($R^2$ = `r lm.1`, *p* < .001) as well as incrementally across the Cattell index ($\Delta{R^2}$ = `r lm.2 - lm.1`, *p* < .001), and also exhibiting an incremental polynomial effect ($\Delta{R^2}$ = `r lm.3 - lm.2`, *p* < .001).

To further elaborate this point, consider, for example, Condition 4 as presented in Table \@ref(tab:Tab1). Here, three groups are characterized by similar distributions of attitudes (normally distributed) and one, Female.B, is characterized by negatively skewed attitudes. The greatest unweighted error here arises from sampling scenarios in which there are many Female.B (e.g., in our specifications, 6,400) and fewer males and Department A females[^4], but the female.b exhibit a much lower response rate (e.g., 20%) than do other groups, who respond at a high rate (e.g., 80%). That is, it is not merely response rate, but response rate within these identifiable groups, and whether or not those response rate differences parallel underlying attitudinal differences that drives sample misrepresentation.

[^4]: Because of the "marginal" versus "cell" specifications of population constituencies, our most extreme example here necessarily results in 400 Male.A's, 1,600 Male.B's, and 1,600 Female.A's. This was a decision based on keeping the population N's at 10,000 and certainly more extreme population constituency combinations could be examined in future like-minded explorations.

# Impact of weighting

Research question 3 was focused on the impact of weights on both biased (e.g., misrepresentative) and unbiased sample estimates[^revisit]. Figure \@ref(fig:Overall4) provides a broad summary of the results across the eight different attitudinal distribution conditions, presenting the average absolute discrepancy from the population mean for the weighted and unweighted sample estimates. Conditions one through three demonstrate that, on average, the unweighted sample mean provides a good (unbiased) estimate of the population mean when the distributional form does not differ across constituent groups (e.g., the distributions of attitudes are of similar functional forms and locations for all constituent groups). This is regardless of form or extent of nonresponse. Additionally, weighting remediates deviations about the true mean in all five attitudinally discrepant conditions, even when substantive relative error exists in the unweighted estimate (e.g., the rightmost bars in Figure \@ref(fig:Overall4)). Although the *patterns* of unweighted sample mean discrepancies differed across conditions, all eight conditions exhibited similar omnibus effect (weighting ameliorating error wherever it arose in the unweighted statistic).

[^revisit]: Come back to this phrasing after decision is made on RQ 3 wording (whether to avoid using the term bias or not).

### Weighting and Sampling Error

Mean square error is a second important index for sample quality. It is well-known that the application of weights increases (random) errors of precision, which was also empirically true in the current study. For each condition in our simulations, we calculated the standard deviations of 40.96 million unweighted and 40.96 million weighted samples means (4,096 possible population-sample combinations by 10,000 iterations), which yielded eight empirically-estimated standard errors of unweighted and weighted sample means. Figure \@ref(fig:Overall4) visually presents these standard errors in eight pairs of bars, demonstrating that the standard error of weighted sample means tended to be 16% to 18% larger than that of unweighted sample means regardless of condition (excluding Conditions 1-3). These errors highlight the caveat that weighting should only be applied in the active nonresponse case (e.g., although the aggregate effect of weighting with passive nonresponse is error-minimizing, any one sampling condition is *more likely* to result in greater deviation from the population parameter when weighting is applied to sample data driven by passive nonresponse).

## Collective roles of response rate, form, and attitudinal distribution

As an aggregate across sampling events, weighting always corrects sample bias when it is present in the unweighted estimate. However, the standard errors suggest that for any *one* sampling event in the absence of bias, the likelihood that the sample mean approximates the *mean* of sample means is (slightly) greater for the unweighted estimate. When bias is present (in the unweighted estimate), there is obviously no advantage to "being closer" to this biased mean of means. That is, under some circumstances, the mean of unweighted sample means does not center on the population mean. The implications of this seem quite obvious: Weighting should only be applied if bias is anticipated in the sample estimate. This may seem to be a picayune recommendation, but we note here that this advocation is not heeded in public opinion polling applications, where the computation and application of weights are default practice (**CITES?** - perhaps AAPOR standards or personal communication with polling agencies such as Gallop).

**Question for David - Can we look at the "crossing point?" (e.g., when MSE becomes excessive)** 

**[perhaps David can derive/find a proof to parallel our results?] (Table 1 + ResponseRate1 + SDForm2 + Figure 4) Maybe try to combine Figures 2 and 3 (put SD on Figure 3 - color code)**


# Discussion

We view nonresponse as a serious problem that should be addressed via repeated attempts to survey particularly reluctant or hard-to-reach respondents because nonresponse may be reasonably expected to be greatest in groups that are most unsatisfied [e.g., @taris_how_2007]. However, several researchers have noted potentially misplaced relative emphasis on response rates, with @cook_meta-analysis_2000, @krosnick_survey_1999, and @visser_mail_1996 articulating the point that representativeness of the sample is more important than response rate. We also believe that the goal in organizational surveying should be representativeness as opposed to exhaustiveness.  @krosnick_survey_1999 specifically comments that, even when probability sampling is employed, response rate does not necessarily implicate either good or poor sample representativeness. One aim of this paper is to stress this primary 'representativeness' orientation to those who may be otherwise inclined to focus on response rate as a sufficient index of quality (while also stressing sample weighting as a practice that can potentially remediate *mis*representativeness).

With the above in mind, we set out to answer three fairly straightforward questions: What roles do 1) response rate and 2) form of nonresponse have on population misrepresentation, and 3) what impact does the application of weights have on the quality of sample estimates? The simulations demonstrate that the impact of (mere) response rate is contingent on the underlying distributions of population attitude. The effect is moderated -- there is not a simple relationship between response rate and misrepresentation. Rather, a wide range of representative/ error-filled estimates can be expected all along the response rate continuum. See Figure \@ref(fig:ResponseRate1). Conditions 1 through 3 are fully immune and all other conditions are occasionally immune to response rate influence.

Regarding question 2, Figure \@ref(fig:SDForm2) shows that the largest misrepresentation sampling scenarios are associated with *greater degrees of* active nonresponse. However, there also exist active nonresponse scenarios within which little or no misrepresentation occurs. Active forms of nonresponse can harm the unweighted sample estimate, but only when the pattern of active nonresponse is accompanied by differing distributions of attitudes within the active nonrespondent "populations" [this would appear to be a reasonable expectation based on the literature, e.g., @rogelberg_employee_2000; @rogelberg_profiling_2003; @spitzmuller_survey_2007]. Weighting "always" helps, as long as you capture the proper strata (which of course we were able to do via controlled simulation). 

The results are presented with at least three primary takeaways: 1) our simulations are comprehensive, iterating through all possible combinations of response rates -- those paralleling population distributions, those inversely mirroring population distributions, and those "orthogonal to" population distributions, 2) the "SD" operationalization of passive to active forms of nonresponse is a bit crude and insensitive to specific combinations of response rates expected to manifest or not manifest in bias, and 3) substantial bias may be present in the unweighted estimate even with only small proportions of active non-response (e.g., only one or two groups exhibiting slightly different response rates, with the resulting discrepancy [population versus sample mean] being quite large). 

It may be noted here that the organizational surveying categorization of passive versus active somewhat parallels the broader statistical focus on data that is missing at random or completely at random [MAR or MCAR, see for example, @heitjan_distinguishing_1996] versus data not missing at random [MNAR, see for example, @enders2011missing]. Imputation is a common remediation technique for data MAR or MCAR whereas MNAR solutions may involve strategies such as latent variable estimation procedures [@muthen_structural_1987]. In the context of organizational surveying, the current findings lead to a similar bifurcation of remediation methods - post-stratification weighting is recommended only in the circumstance of active nonresponse.

It has been stated that active nonresponse is relatively harmless unless the actively nonrespondent group is relatively large [the proportion of active nonrespondents is higher than 15%; @dooley2003handling; @rogelberg_introduction_2007; @rogelberg_profiling_2003; @werner_reporting_2007]. Note here however, the possible disconnect between the reports of 15% active nonresponse and declining response rates (trending toward 50%). Certainly with decreasing overall response rates, the likely reasons would appear to be more active than passive (e.g., it is difficult to entertain the idea that potential respondents are more likely to forget to respond today than they were 40 years ago). Although the weighted mean proved an unbiased estimate of the population mean across all simulations, in circumstances where no bias existed in the unweighted estimate, the trade-off between bias-correction and random error of precision (e.g., standard error) also needs to be acknowledged.

The current findings are of course qualified by the uniqueness of our simulations, most notably our ability to fully capture the correct population parameters (e.g., because these were "created" by us, we were also able to identify these strata as the nonresponse contributors). Even in the extreme conditions (e.g., a small "population" with a correspondingly low response rate), the weighting algorithm was able to provide a bias correction. This is undoubtedly attributable to our random sampling procedure (instead of, for example, sampling conditionally from the population distributions), but here we do note that the raking procedure is applied at the "margins" (e.g., variable level, not intersectional level), although our introduction of a biasing element is at the cell (intersection) level.

## Future Directions

There is of course no need to restrict weighting protocols to demographic groups. Organizational surveyors have a rich tradition of attending to drivers of nonresponse [see, for example, @rogelberg_introduction_2007]. Groupings of any sort can be the basis of weighting (for example, pre-survey probing might assign probabilities of nonresponse, and these probabilities can be retained post-administration as weighting guides). 

Our operationalization of passive nonresponse was based on realized subsample differences in response rate. Of course it is plausible that consistent response rates (e.g., 36%, 36%, 36%, 36%) could have corresponding *non-sampled* elements who represent active non-response. Our methodology did not model these scenarios, but future like-minded investigations may wish to do so.

There may also be some important implications here regarding sample (and population) size. Because organizational surveyors likely interface with organizations of varying sizes (perhaps some of which are small- or medium-sized), the implications of our simulations particularly in the small population conditions, were highlighted. Findings specific to these conditions were: XXX, XXX, XXX.

*Research question placeholder*: What are the important interrelationships between nonresponse form, response rate, and underlying distributional attributes that impact population misrepresentation?

A very practical implication of this study is that future organizational researchers may find more success implementing strategic sampling strategies as opposed to (or in addition to) pursuing response enhancement. That is, as a field, organizational researchers have been focused on response-enhancing strategies that minimize the presence of nonresponse. The current findings suggest that more careful adherence to random sampling from carefully constructed population frames may provide a different route to the same end-goal of sample representativeness.

Experimental methods within the psychological discipline have long been criticized for heavy reliance on samples of convenience (for instance, student samples). Very little progress has been made regarding the application of appropriate population sampling procedures in experimentation. Certain non-experimental procedures (most notably organizational surveying) hold paradoxical advantage over experimental procedures primarily in this arena of sampling - particularly in consideration of population coverage, which refers to the percent of a population that is reachable by the sampling procedure (e.g., postal, intra-office, or internet invitation) and likelihood of having access to population parameter estimates (e.g., strata constituencies).  There is a rich tradition and literature of public opinion polling procedures and techniques from which to draw. These procedures, however, only hold advantage if the non-experimental methodologist acknowledges the criticality of sample representativeness. The current paper provides one corrective technique (post-stratification weighting) as an important focus for the organizational surveyor who shares this primary interest in maximizing sample representativeness.

We note the above "advantage" held by organizational surveyors because extensions of the current protocol include investigating how inaccurate census estimates (and/or grabbing the "wrong" group) affects the quality of sample estimates. That is, in our controlled simulations, we were able to know population constituencies, because they were set by us! In real-world applications, there is likely more error between the population estimate and actual population constituency. Similarly, if the association between attitude and group membership were to be controlled, there may be conditions identified whereby weighting loses its efficacy (e.g., low "correlations" between attitude and group membership). Future simulations should test boundary conditions for this type of error, identifying at what point inaccuracy in the population constituency estimate appreciably degrades the weighting procedure. Furthermore, it was demonstrated here that, when bias exists, weighting corrects it. Weighting also, however, results in a larger mean square error (MSE; expected spread of sample estimates around the population parameter). Feasibly then, there is a point at which the decreased bias is accompanied by an unacceptably inflated MSE. At which point does this occur? This is another fertile area for future exploration.

Most potential issues with weighting are addressed through careful consideration of the appropriate strata to take under consideration as well as ultimate level of aggregation (what group constitutes the population of interest or focus of feedback; e.g., regional, functional, or organizational?). We recommend the surveyor especially considers groups that might have issues of active forms of nonresponse and collect those demographics so weighting is an option. It is particularly in these contexts of 'unsatisfied' employees being less likely to respond to surveys that pre-stratification consideration becomes critical (for instance, if there is an inclination that attitudes may differ across, for example, night versus day shift workers, it is important that shift be measured and incorporated as a stratum prior to survey administration).

For Condition 5 (for example, low/high response rates with minority/majority population constituencies). The lower-right to upper-left diagonal reflects response rates that parallel population constituencies. The patterns across these stressors were consistent, with the weighted sample means (red dots) providing unbiased estimates of the population parameter, whereas the unweighted sample means (grey dots) tended to yield unbiased estimates when the population constituencies were roughly equal (e.g., close to 50%/50%).

Figure 3 drills down this information further by extracting unweighted and weighted estimates in one specific marginal population parameter combination (here, 60% males and 40% females; 40% in department A and 60% in department B). In doing so, the population parameters were in control and sample parameters were set free (see dotted red rectangle in Figure 2). Therefore, Figure 3 was then arranged in a fashion that allowed further investigation into the interactive effect of marginal sample parameters (gender on the x-axis and department on the y-axis) on the effectiveness of post-stratification weighting reflected by the pattern of grey and red dots. **Huh? - find old version or delete**

Could be introducing more error if try to apply weights to correct constintuent proportionalities with passive nonresponse.

Mention tradition of single-item indicators in public opinion polling versus multi-item scales in Psychological assessment?

> PRIOR TO RQs: after chatting with Yang (10/31/19) these need to be clarified a bit - reading 11/3 they make sense but need to be read very carefully. Check with Yang on 1/26 Skype. 2/1 revisions seem ok to Kulas. Three moving parts: underlying attitudinal distributions, response rate, and form of nonresponse <- perhaps we should make these variables more explicit prior to the procedure/results...

Our operationalization of active nonresponse as subgroup differences in response rates of course merits validation. The literature suggests that individuals with... this 
Whether or not subgroup differences in response rate can (or should) be investigated as potential indication of active nonresponse is an empirical question and future investigations would benefit from exploring the extent to which such variability in simple response rate across constituent groups *should* be interpreted as indicative of active nonresponse. This would be an extension of @taris_how_2007, who noted that selection of an individual population element into a realized sample may in fact be predictable (because of, for example, an increased likelihood of not responding when dissatisfied or disgruntled). This operationalization is dependent on subgroup comparison (e.g., is not reflective of an entire organization that collectively exhibits active nonresponse).

Most likely nonrespondents are actually those in the middle (not extremely dissatisfied or extremely satisfied). 

>"put differently, a high response rate may not allow for valid inferences and a lower response rate might adqueately represent the broader population" [p. 1574; @holtom2022survey].

Previous presentations have noted that bias is sometimes associated with nonresponse and othertimes it is not - this research has not been explicit in the specific conditions that moderate this association, however. The current paper does make this association explicit. It is not merely the form of nonresponse that determines whether or not bias occurs, but also the underlying distributions that the response probabilities are applied to.  Some distributional patterns are immune to the biasing effects of active nonresponse (see, for example, Conditions 1 through 3). Some patterns of active nonresponse also result in no bias even when distributional patterns deviate substantially (see, for example, Condition 8 where a 20%, 20%, 80%, 80% response rate pattern exhibits no error). The target therefore should not be merely form of nonresponse but also underlying attitudes. Regardless, however, weighting always remediates the error when it occurs (and does not add error where it is absent).


> Integration of IT/IS systems within HR functions hopefully assists the (un)likelihood that organizatinoal population frames are either deficient or contaminated, although we note that this possibility (frame misspecification) is much more plausible within organziations that do not have updated or integrated HR IT/IS systems (perhaps, ironically, *smaller* organizations). 

\newpage

# References
```{r create_r-references}
r_refs(file = "simulation paper references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

